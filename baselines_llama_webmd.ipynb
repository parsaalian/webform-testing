{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "806f75bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T22:29:57.552936Z",
     "start_time": "2023-07-27T22:29:56.099071Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'create_driver_server' from 'method.ours.utils' (/home/webtesting/webform-testing/method/ours/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupport\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m expected_conditions \u001b[38;5;28;01mas\u001b[39;00m EC\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmethod\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_random_values, generate_static_values, generate_llm_values, generate_llm_values_llama\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmethod\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mours\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_driver_server, get_xpath, interact_with_input\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmethod\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mours\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhistory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HistoryTable\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmethod\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mours\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeedback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_global_feedback\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'create_driver_server' from 'method.ours.utils' (/home/webtesting/webform-testing/method/ours/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from method.models import generate_random_values, generate_static_values, generate_llm_values, generate_llm_values_llama\n",
    "from method.ours.utils import create_driver, get_xpath, interact_with_input\n",
    "from method.ours.history import HistoryTable\n",
    "from method.ours.feedback import get_global_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8407538",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T22:30:03.813204Z",
     "start_time": "2023-07-27T22:30:03.743291Z"
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Global Variables\n",
    "HEADLESS = False\n",
    "TEXT_EMBEDDING_METHOD = 'ADA' # ['ADA', 'WORD2VEC', 'SPACY']\n",
    "GRAPH_EMBEDDING_METHOD = 'NODE2VEC' # ['NODE2VEC', 'GCN']\n",
    "\n",
    "# URL = 'https://www.ups.com/ca/en/Home.page'\n",
    "# 'https://www.healthgrades.com/find-a-doctor'\n",
    "# 'https://www.babycenter.com/child-height-predictor'\n",
    "# 'https://www.babycenter.com/pregnancy-weight-gain-estimator'\n",
    "# 'https://www.yelp.com/'\n",
    "# URL = 'https://www.aircanada.com/ca/en/aco/home.html'\n",
    "# 'https://app.invoicing.co/#/register'\n",
    "# 'http://localhost:8080/1/common/items/create'\n",
    "# 'http://localhost:3000/default-channel/en-US/account/register/'\n",
    "# 'http://localhost:9000/dashboard/discounts/sales/add'\n",
    "# 'http://localhost:9000/dashboard/customers/add'\n",
    "# 'https://www.mbta.com/'\n",
    "# 'https://www.united.com/en/us'\n",
    "# 'https://www.uhaul.com/Truck-Rentals/'\n",
    "\n",
    "# URL = 'https://seatgeek.ca/'\n",
    "# 'https://www.stubhub.ca/'\n",
    "# 'https://www.thetrainline.com/en-us'\n",
    "# 'https://resy.com/'\n",
    "# 'https://www.aa.com/homePage.do' \n",
    "# 'https://www.jetblue.com/'\n",
    "# 'https://www.united.com/en/us'\n",
    "# 'https://www.tripadvisor.ca/'\n",
    "# 'https://www.webmd.com/drugs/2/index'\n",
    "# 'https://www.thumbtack.com/'\n",
    "# 'https://www.rei.com/'\n",
    "# 'https://www.carmax.com/'\n",
    "# 'https://www.amazon.ca/'\n",
    "# URL = 'https://www.macys.com/'\n",
    "URL = 'https://www.webmd.com/drugs/2/index'\n",
    "\n",
    "def get_to_form(driver):\n",
    "    try:\n",
    "        driver.get(URL)\n",
    "    except:\n",
    "        print('timeout')\n",
    "    \n",
    "    # UPS - Quote\n",
    "    #time.sleep(2)\n",
    "    #WebDriverWait(driver, 10).until(\n",
    "    #    EC.presence_of_element_located((By.ID, 'tabs_0_tab_1'))\n",
    "    #).click()\n",
    "\n",
    "    time.sleep(2)\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((\n",
    "            By.XPATH,\n",
    "            '//BODY/DIV[1]/DIV[1]/DIV[2]/DIV[3]/DIV[1]/DIV[2]/DIV[1]/DIV[1]')\n",
    "        )\n",
    "    ).click()\n",
    "    \n",
    "    # AC - Multi-city\n",
    "    #time.sleep(2)\n",
    "    #WebDriverWait(driver, 5).until(\n",
    "    #    EC.presence_of_element_located((By.ID, 'bkmgFlights_tripTypeSelector_M'))\n",
    "    #).click()    \n",
    "    \n",
    "    '''\n",
    "    # Budget - Reservation\n",
    "    WebDriverWait(driver, 5).until(\n",
    "        EC.presence_of_element_located((By.ID, 'PicLoc_value'))\n",
    "    ).click()\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    # AC - My Bookings\n",
    "    time.sleep(2)\n",
    "    WebDriverWait(driver, 5).until(\n",
    "        EC.presence_of_element_located((By.ID, 'bkmg-tab-button-mngBook'))\n",
    "    ).click()\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # AC - Multi-city\n",
    "    time.sleep(2)\n",
    "    WebDriverWait(driver, 5).until(\n",
    "        EC.presence_of_element_located((By.ID, 'bkmgFlights_tripTypeSelector_M'))\n",
    "    ).click()\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # Akaunting\n",
    "    try:\n",
    "        time.sleep(2)\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((\n",
    "                By.ID,\n",
    "                'email')\n",
    "            )\n",
    "        ).send_keys('me@company.com')\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((\n",
    "                By.ID,\n",
    "                'password')\n",
    "            )\n",
    "        ).send_keys('password')\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((\n",
    "                By.XPATH,\n",
    "                '//*[@type=\"submit\"]')\n",
    "            )\n",
    "        ).click()\n",
    "    except:\n",
    "        pass\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # Saleor\n",
    "    try:\n",
    "        time.sleep(2)\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((\n",
    "                By.XPATH,\n",
    "                '//BODY/DIV[1]/MAIN[1]/DIV[1]/DIV[1]/DIV[1]/FORM[1]/DIV[2]/DIV[1]/INPUT[1]')\n",
    "            )\n",
    "        ).send_keys('admin@example.com')\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((\n",
    "                By.XPATH,\n",
    "                '//BODY/DIV[1]/MAIN[1]/DIV[1]/DIV[1]/DIV[1]/FORM[1]/DIV[4]/DIV[1]/DIV[1]/INPUT[1]')\n",
    "            )\n",
    "        ).send_keys('admin')\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((\n",
    "                By.XPATH,\n",
    "                '//BODY/DIV[1]/MAIN[1]/DIV[1]/DIV[1]/DIV[1]/FORM[1]/DIV[5]/BUTTON[1]')\n",
    "            )\n",
    "        ).click()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((\n",
    "            By.XPATH,\n",
    "            '//BODY/DIV[1]/MAIN[1]/DIV[1]/DIV[2]/MAIN[1]/FORM[1]/DIV[1]/DIV[2]/DIV[4]/DIV[2]/LABEL[1]/SPAN[1]/SPAN[1]/INPUT[1]')\n",
    "        )\n",
    "    ).click()\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # Saleor\n",
    "    try:\n",
    "        time.sleep(2)\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((\n",
    "                By.XPATH,\n",
    "                '//BODY/DIV[1]/MAIN[1]/DIV[1]/DIV[1]/DIV[1]/FORM[1]/DIV[2]/DIV[1]/INPUT[1]')\n",
    "            )\n",
    "        ).send_keys('admin@example.com')\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((\n",
    "                By.XPATH,\n",
    "                '//BODY/DIV[1]/MAIN[1]/DIV[1]/DIV[1]/DIV[1]/FORM[1]/DIV[4]/DIV[1]/DIV[1]/INPUT[1]')\n",
    "            )\n",
    "        ).send_keys('admin')\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((\n",
    "                By.XPATH,\n",
    "                '//BODY/DIV[1]/MAIN[1]/DIV[1]/DIV[1]/DIV[1]/FORM[1]/DIV[5]/BUTTON[1]')\n",
    "            )\n",
    "        ).click()\n",
    "    except:\n",
    "        pass\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    time.sleep(2)\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((\n",
    "            By.XPATH,\n",
    "            '//BODY/DIV[1]/DIV[2]/MAIN[1]/DIV[1]/DIV[1]/DIV[1]/DIV[2]')\n",
    "        )\n",
    "    ).click()\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # Pet Clinic - Add Owner\n",
    "    time.sleep(2)\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((\n",
    "            By.XPATH,\n",
    "            '//BODY/APP-ROOT[1]/DIV[1]/NAV[1]/DIV[1]/UL[1]/LI[2]')\n",
    "        )\n",
    "    ).click()\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((\n",
    "            By.XPATH,\n",
    "            '//BODY/APP-ROOT[1]/DIV[1]/NAV[1]/DIV[1]/UL[1]/LI[2]/UL[1]/LI[2]')\n",
    "        )\n",
    "    ).click()\n",
    "    '''\n",
    "\n",
    "\n",
    "def find_form():\n",
    "    return WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((\n",
    "            By.XPATH,\n",
    "            '//BODY/DIV[1]/DIV[1]/DIV[2]/DIV[3]/DIV[1]/DIV[2]/DIV[1]/DIV[1]'\n",
    "        ))\n",
    "    )\n",
    "\n",
    "def find_button():\n",
    "    '''\n",
    "    return find_form().find_element(\n",
    "        By.XPATH,\n",
    "        '/html/body/div[1]/div/div[2]/div[3]/div/div[2]/div/div[1]/div/div/div/div'\n",
    "    )\n",
    "    '''\n",
    "    # return driver.find_elements(By.TAG_NAME, 'button')[10]\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc0e906c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T22:30:04.064728Z",
     "start_time": "2023-07-27T22:30:04.016575Z"
    }
   },
   "outputs": [],
   "source": [
    "METHOD = 'llama'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8ec6672",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T22:30:13.696699Z",
     "start_time": "2023-07-27T22:30:04.577360Z"
    }
   },
   "outputs": [],
   "source": [
    "driver = create_driver_server(HEADLESS)\n",
    "get_to_form(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96cabf9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T22:30:15.639830Z",
     "start_time": "2023-07-27T22:30:15.381049Z"
    }
   },
   "outputs": [],
   "source": [
    "html = driver.find_element(By.TAG_NAME, 'html').get_attribute('outerHTML')\n",
    "form = find_form()\n",
    "form_xpath = get_xpath(driver, form)\n",
    "\n",
    "inputs = form.find_elements(\n",
    "    By.XPATH,\n",
    "    f'{form_xpath}//input | {form_xpath}//textarea | {form_xpath}//select'\n",
    ")\n",
    "\n",
    "inputs = list(filter(\n",
    "    lambda x: x.get_attribute('type') != 'hidden' and x.get_attribute('hidden') != 'true',\n",
    "    inputs\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "668d49d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T22:30:17.361825Z",
     "start_time": "2023-07-27T22:30:17.307755Z"
    }
   },
   "outputs": [],
   "source": [
    "history_table = HistoryTable(\n",
    "    url=URL,\n",
    "    xpath=form_xpath\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "026d92c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/webtesting/miniconda3/envs/llama2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The safetensors archive passed at /home/webtesting/.cache/huggingface/hub/models--TheBloke--Llama-2-70B-chat-GPTQ/snapshots/0829129a5b611f47fda36ee12d8c2a39c30bf05c/gptq_model-4bit--1g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.                                                                                    \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "def load_model_llama(model_type, use_triton=False):\n",
    "    if model_type == '70B':\n",
    "        model_name_or_path = \"TheBloke/Llama-2-70B-chat-GPTQ\"\n",
    "        model_basename = \"gptq_model-4bit--1g\"\n",
    "    elif model_type == '13B':\n",
    "        model_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
    "        model_basename = \"gptq_model-4bit-128g\"\n",
    "    else:\n",
    "        raise ValueError('Invalid model_type. Choose either \"70B\" or \"13B\".')\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True, device_map=\"auto\")\n",
    "\n",
    "    model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "                                               #model_basename=model_basename,\n",
    "                                               revision = \"main\",\n",
    "                                               inject_fused_attention=False,\n",
    "                                               use_safetensors=True,\n",
    "                                               trust_remote_code=False,\n",
    "                                               use_triton=use_triton,\n",
    "                                               quantize_config=None)\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "# Load the model - load_model_llama2_70b\n",
    "tokenizer, model = load_model_llama(\"70B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aa02c36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T22:30:38.393454Z",
     "start_time": "2023-07-27T22:30:17.915548Z"
    }
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.86 GiB (GPU 0; 23.69 GiB total capacity; 17.76 GiB already allocated; 4.49 GiB free; 18.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     values \u001b[38;5;241m=\u001b[39m generate_random_values(driver, inputs)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m METHOD \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 7\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_llm_values_llama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_attribute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mouterHTML\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     values \u001b[38;5;241m=\u001b[39m generate_llm_values(form\u001b[38;5;241m.\u001b[39mget_attribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mouterHTML\u001b[39m\u001b[38;5;124m'\u001b[39m), openai\u001b[38;5;241m.\u001b[39mapi_key)\n",
      "File \u001b[0;32m~/webform-testing/implementation/method/models/llama.py:29\u001b[0m, in \u001b[0;36mgenerate_llm_values_llama\u001b[0;34m(form, model, tokenizer)\u001b[0m\n\u001b[1;32m     20\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124m[INST] <<SYS>>\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLLM_value_generation_prompt\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124m<</SYS>>\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124mDO NOT EXPLAIN YOUR ANSWERS. [/INST]\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     28\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer(prompt_template, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 29\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:438\u001b[0m, in \u001b[0;36mBaseGPTQForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"shortcut for model.generate\"\"\"\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode(), torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype):\n\u001b[0;32m--> 438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/generation/utils.py:1538\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1532\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1533\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_return_sequences has to be 1 when doing greedy search, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1534\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1535\u001b[0m         )\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[1;32m   1552\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/generation/utils.py:2362\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2359\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2361\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2362\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2363\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2365\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2366\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2367\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2370\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:806\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    803\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 806\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:693\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    685\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    686\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    687\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    691\u001b[0m     )\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 693\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:408\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    405\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:330\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    327\u001b[0m key_states \u001b[38;5;241m=\u001b[39m repeat_kv(key_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n\u001b[1;32m    328\u001b[0m value_states \u001b[38;5;241m=\u001b[39m repeat_kv(value_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n\u001b[0;32m--> 330\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_weights\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, q_len, kv_seq_len):\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention weights should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(bsz,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\u001b[38;5;250m \u001b[39mq_len,\u001b[38;5;250m \u001b[39mkv_seq_len)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_weights\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    336\u001b[0m     )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.86 GiB (GPU 0; 23.69 GiB total capacity; 17.76 GiB already allocated; 4.49 GiB free; 18.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "values = None\n",
    "if METHOD == 'static':\n",
    "    values = generate_static_values(driver, inputs)\n",
    "elif METHOD == 'random':\n",
    "    values = generate_random_values(driver, inputs)\n",
    "elif METHOD == 'llama':\n",
    "    values = generate_llm_values_llama(form.get_attribute('outerHTML'), model, tokenizer)    \n",
    "else:\n",
    "    values = generate_llm_values(form.get_attribute('outerHTML'), openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d251b320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaGPTQForCausalLM(\n",
       "  (model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(32000, 8192, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-79): 80 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "            (k_proj): GeneralQuantLinear(in_features=8192, out_features=1024, bias=True)\n",
       "            (o_proj): GeneralQuantLinear(in_features=8192, out_features=8192, bias=True)\n",
       "            (q_proj): GeneralQuantLinear(in_features=8192, out_features=8192, bias=True)\n",
       "            (v_proj): GeneralQuantLinear(in_features=8192, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (act_fn): SiLUActivation()\n",
       "            (down_proj): GeneralQuantLinear(in_features=28672, out_features=8192, bias=True)\n",
       "            (gate_proj): GeneralQuantLinear(in_features=8192, out_features=28672, bias=True)\n",
       "            (up_proj): GeneralQuantLinear(in_features=8192, out_features=28672, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=8192, out_features=32000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8476d5c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-27T22:30:57.755662Z",
     "start_time": "2023-07-27T22:30:57.689284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ad491e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T18:46:03.976669Z",
     "start_time": "2023-07-25T18:46:03.922316Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_element(element_id):\n",
    "    return driver.find_element(By.ID if METHOD == 'llm' else By.XPATH, element_id)\n",
    "\n",
    "\n",
    "def send_values(elemend_ids, values):\n",
    "    for element_id in elemend_ids:\n",
    "        try:\n",
    "            element = get_element(element_id)\n",
    "        except:\n",
    "            continue\n",
    "        value = values[element_id]\n",
    "        \n",
    "        element_type = element.get_attribute('type') or 'text'\n",
    "    \n",
    "        if element_type in ['radio', 'checkbox'] or element.tag_name in ['button']:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            interact_with_input(element, value)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        \n",
    "def submit_form(form):\n",
    "    # submit = driver.find_element(By.XPATH, f'{form_xpath}//*[@type=\"submit\"]')\n",
    "    # driver.find_element(By.TAG_NAME, 'body').click()\n",
    "    submit = find_button()\n",
    "    \n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            interact_with_input(submit, True)\n",
    "            time.sleep(0.5)\n",
    "        except:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "866d934b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-25T18:48:50.943913Z",
     "start_time": "2023-07-25T18:47:00.919211Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m passing_set \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 2\u001b[0m     element_id: values[element_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassing\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m element_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m()\n\u001b[1;32m      3\u001b[0m }\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element_id, element_values \u001b[38;5;129;01min\u001b[39;00m values\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      7\u001b[0m     get_to_form(driver)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "passing_set = {\n",
    "    element_id: values[element_id]['passing'] for element_id in values.keys()\n",
    "}\n",
    "\n",
    "\n",
    "for element_id, element_values in values.items():\n",
    "    get_to_form(driver)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    try:\n",
    "        element = get_element(element_id)\n",
    "    except:\n",
    "        continue\n",
    "    element_type = element.get_attribute('type') or 'text'\n",
    "    \n",
    "    if element_type in ['radio', 'checkbox', 'submit']:\n",
    "        continue\n",
    "    \n",
    "    # print(element)\n",
    "    \n",
    "    for failing_value in element_values['failing']:\n",
    "        # try:\n",
    "        get_to_form(driver)\n",
    "        time.sleep(1)\n",
    "    \n",
    "        passing_copy = copy.copy(passing_set)\n",
    "        passing_copy[element_id] = failing_value\n",
    "        send_values(values.keys(), passing_copy)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        submit_form(form)\n",
    "\n",
    "        new_html = driver.find_element(By.TAG_NAME, 'body').get_attribute('outerHTML')\n",
    "        global_feedback = get_global_feedback(html, new_html, remove_form_children=False)\n",
    "\n",
    "        history_table.add(\n",
    "            passing_copy,\n",
    "            'base',\n",
    "            global_feedback,\n",
    "            driver.current_url\n",
    "        )\n",
    "        # except Exception as e:\n",
    "        #     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "025351a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T16:20:34.961485Z",
     "start_time": "2023-07-24T16:20:34.887290Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, v in enumerate(history_table.table['values']):\n",
    "    jv = json.loads(v)\n",
    "    for key, value in jv.items():\n",
    "        if key not in history_table.table.columns:\n",
    "            history_table.table[key] = None\n",
    "        history_table.table.loc[i, key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7af1f1b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T03:03:22.906817Z",
     "start_time": "2023-07-24T03:03:22.839924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>values</th>\n",
       "      <th>variation_type</th>\n",
       "      <th>feedback</th>\n",
       "      <th>new_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [values, variation_type, feedback, new_url]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_table.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7263ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
